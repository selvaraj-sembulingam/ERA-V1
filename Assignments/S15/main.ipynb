{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/selvaraj-sembulingam/ERA-V1.git\n%cd ERA-V1/Assignments/S15","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-01T16:46:19.858369Z","iopub.execute_input":"2023-09-01T16:46:19.859346Z","iopub.status.idle":"2023-09-01T16:46:37.115480Z","shell.execute_reply.started":"2023-09-01T16:46:19.859301Z","shell.execute_reply":"2023-09-01T16:46:37.114315Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'ERA-V1'...\nremote: Enumerating objects: 1899, done.\u001b[K\nremote: Counting objects: 100% (797/797), done.\u001b[K\nremote: Compressing objects: 100% (507/507), done.\u001b[K\nremote: Total 1899 (delta 365), reused 642 (delta 282), pack-reused 1102\u001b[K\nReceiving objects: 100% (1899/1899), 246.87 MiB | 18.75 MiB/s, done.\nResolving deltas: 100% (814/814), done.\n/kaggle/working/ERA-V1/Assignments/S15\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q datasets tokenizers torchmetrics pytorch_lightning ","metadata":{"execution":{"iopub.status.busy":"2023-09-01T16:47:07.247437Z","iopub.execute_input":"2023-09-01T16:47:07.247830Z","iopub.status.idle":"2023-09-01T16:47:20.174166Z","shell.execute_reply.started":"2023-09-01T16:47:07.247801Z","shell.execute_reply":"2023-09-01T16:47:20.172890Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%%writefile datamodule.py\nimport pytorch_lightning as pl\nfrom dataset import BilingualDataset\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom pathlib import Path\n\nfrom datasets import load_dataset\nfrom tokenizers import Tokenizer\nfrom tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\n\nclass OpusBookDataModule(pl.LightningDataModule):\n  def __init__(self, config):\n    super().__init__()\n    self.config = config\n\n  def get_all_sentences(self, ds, lang):\n    for item in ds:\n      yield item[\"translation\"][lang]\n\n  def get_or_build_tokenizer(self, ds, lang):\n    tokenizer_path = Path(self.config[\"tokenizer_file\"].format(lang))\n    if not Path.exists(tokenizer_path):\n      tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n      tokenizer.pre_tokenizer = Whitespace()\n      trainer = WordLevelTrainer(\n        special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2\n      )\n      tokenizer.train_from_iterator(self.get_all_sentences(ds, lang), trainer=trainer)\n      tokenizer.save(str(tokenizer_path))\n    else:\n      tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer\n    \n  def setup(self, stage):\n      # It only has the train split, so we divide it ourselves\n      ds_raw = load_dataset(\n        \"opus_books\", f\"{self.config['lang_src']}-{self.config['lang_tgt']}\", split=\"train\"\n      )\n\n      # Build tokenizer\n      self.tokenizer_src = self.get_or_build_tokenizer(ds_raw, self.config[\"lang_src\"])\n      self.tokenizer_tgt = self.get_or_build_tokenizer(ds_raw, self.config[\"lang_tgt\"])\n\n      # Keep 90% for training, 10% for validation\n      train_ds_size = int(0.9 * len(ds_raw))\n      val_ds_size = len(ds_raw) - train_ds_size\n      train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n\n      self.train_ds = BilingualDataset(\n        train_ds_raw,\n        self.tokenizer_src,\n        self.tokenizer_tgt,\n        self.config[\"lang_src\"],\n        self.config[\"lang_tgt\"],\n        self.config[\"seq_len\"],\n      )\n      self.val_ds = BilingualDataset(\n        val_ds_raw,\n        self.tokenizer_src,\n        self.tokenizer_tgt,\n        self.config[\"lang_src\"],\n        self.config[\"lang_tgt\"],\n        self.config[\"seq_len\"],\n      )\n\n      # Find the maximum length of each sentence in the source and target sentence\n      max_len_src = 0\n      max_len_tgt = 0\n\n      for item in ds_raw:\n        src_ids = self.tokenizer_src.encode(item[\"translation\"][self.config[\"lang_src\"]]).ids\n        tgt_ids = self.tokenizer_tgt.encode(item[\"translation\"][self.config[\"lang_tgt\"]]).ids\n        max_len_src = max(max_len_src, len(src_ids))\n        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n\n      print(f\"Max length of source sentence: {max_len_src}\")\n      print(f\"Max length of target sentence: {max_len_tgt}\")\n\n      return self.tokenizer_src, self.tokenizer_tgt\n\n\n\n  def train_dataloader(self):     \n      return DataLoader(self.train_ds, batch_size=self.config['batch_size'], shuffle=True)  \n    \n  def val_dataloader(self):\n      return DataLoader(self.val_ds, batch_size=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T16:47:20.177171Z","iopub.execute_input":"2023-09-01T16:47:20.177583Z","iopub.status.idle":"2023-09-01T16:47:20.187443Z","shell.execute_reply.started":"2023-09-01T16:47:20.177549Z","shell.execute_reply":"2023-09-01T16:47:20.186457Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing datamodule.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile litmodel.py\nimport torch\nimport torch.nn as nn\nimport torchmetrics\nimport pytorch_lightning as pl\nimport numpy \nfrom dataset import causal_mask\n\nclass LightningTransformer(pl.LightningModule):\n    def __init__(self, model, learning_rate, tokenizer_src, tokenizer_tgt, max_len, num_examples):\n        super(LightningTransformer, self).__init__()\n        self.learning_rate = learning_rate\n        self.model = model\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.tokenizer_src.token_to_id(\"[PAD]\"), label_smoothing=0.1)\n        self.max_len = max_len\n        self.source_texts = []\n        self.expected = []\n        self.predicted = [] \n        self.num_examples = num_examples   \n        self.cer_metric = torchmetrics.text.CharErrorRate()\n        self.wer_metric = torchmetrics.text.WordErrorRate()\n        self.bleu_metric = torchmetrics.text.BLEUScore()\n        self.save_hyperparameters(ignore=['model'])   \n\n    def training_step(self, batch, batch_idx):\n        encoder_input = batch['encoder_input']\n        decoder_input = batch['decoder_input']\n        encoder_mask = batch['encoder_mask']\n        decoder_mask = batch['decoder_mask']\n        encoder_output = self.model.encode(encoder_input, encoder_mask)\n        decoder_output = self.model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n        proj_output = self.model.project(decoder_output)\n        label = batch['label']\n        loss = self.loss_fn(proj_output.view(-1, self.tokenizer_tgt.get_vocab_size()), label.view(-1))\n\n        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=True)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        if batch_idx < self.num_examples:\n            encoder_input = batch['encoder_input']\n            encoder_mask = batch['encoder_mask']\n\n            assert encoder_input.size(0) == 1, \"Batch size must be 1 for validation\"\n\n            model_out = self.greedy_decode(encoder_input, encoder_mask)\n            source_text = batch[\"src_text\"][0]\n            target_text = batch[\"tgt_text\"][0]\n            model_out_text = self.tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n            self.source_texts.append(source_text)\n            self.expected.append(target_text)\n            self.predicted.append(model_out_text)\n\n            # Print the source, target, and model output\n            print(\"-\"*10)\n            print(f\"{f'SOURCE: ':>12}{source_text}\")\n            print(f\"{f'TARGET: ':>12}{target_text}\")\n            print(f\"{f'PREDICTED: ':>12}{model_out_text}\")\n\n        if batch_idx == self.num_examples-1:\n            cer = self.cer_metric(self.predicted, self.expected)\n            self.log(\"val_cer\", cer)\n\n            wer = self.wer_metric(self.predicted, self.expected)\n            self.log(\"val_wer\", wer)   \n\n            bleu = self.bleu_metric(self.predicted, self.expected) \n            self.log(\"val_bleu\", bleu)\n\n        if batch_idx >= self.num_examples:\n            pass\n\n    def greedy_decode(self, source, source_mask):\n        sos_idx = self.tokenizer_tgt.token_to_id(\"[SOS]\")\n        eos_idx = self.tokenizer_tgt.token_to_id(\"[EOS]\")\n\n        # Precompute the encoder output and reuse it for every step\n        encoder_output = self.model.encode(source, source_mask)\n        # Initialize the decoder input with the sos token_to_id\n        decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source)\n        while True:\n          if decoder_input.size(1) == self.max_len:\n            break\n\n          # build mask for target\n          decoder_mask = (\n            causal_mask(decoder_input.size(1)).type_as(source_mask)\n          )\n\n          # Calculate output\n          out = self.model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n\n          # get next token_to_id\n          prob = self.model.project(out[:, -1])\n          _, next_word = torch.max(prob, dim=1)\n          decoder_input = torch.cat(\n            [\n              decoder_input,\n              torch.empty(1, 1).type_as(source).fill_(next_word.item()),\n            ],\n            dim=1,\n          )\n\n          if next_word == eos_idx:\n            break\n        return decoder_input.squeeze(0)\n\n    def test_step(self, batch, batch_idx):\n        pass\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, eps=1e-9)\n        return optimizer","metadata":{"execution":{"iopub.status.busy":"2023-09-01T16:47:20.188839Z","iopub.execute_input":"2023-09-01T16:47:20.189165Z","iopub.status.idle":"2023-09-01T16:47:20.202108Z","shell.execute_reply.started":"2023-09-01T16:47:20.189135Z","shell.execute_reply":"2023-09-01T16:47:20.201155Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Writing litmodel.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport warnings\nimport os\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning import Trainer, seed_everything\n\nfrom datamodule import OpusBookDataModule\nfrom litmodel import LightningTransformer\nfrom config import get_config\ncfg = get_config()\n\n\nfrom model import build_transformer\n\nif __name__ == '__main__':\n  warnings.filterwarnings(\"ignore\")\n  torch.cuda.empty_cache()\n  # os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:12240\"\n\n  pl.seed_everything(1, workers=True)\n  data_module = OpusBookDataModule(cfg)\n  tokenizer_src, tokenizer_tgt = data_module.setup(stage=\"None\")\n  model = build_transformer(\n        tokenizer_src.get_vocab_size(), \n        tokenizer_tgt.get_vocab_size(),\n        cfg[\"seq_len\"],\n        cfg[\"seq_len\"],\n        d_model=cfg[\"d_model\"],\n  )  \n  \n  litmodel = LightningTransformer(model=model, learning_rate=cfg[\"lr\"], tokenizer_src=tokenizer_src, tokenizer_tgt=tokenizer_tgt, max_len=cfg[\"seq_len\"], num_examples=2)\n\n\n  trainer = pl.Trainer(\n      max_epochs=10,\n      deterministic=True,\n      logger=True,\n      enable_model_summary=False,\n      log_every_n_steps=1,\n      precision=16\n  )\n  trainer.fit(litmodel, data_module)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-01T16:47:20.204668Z","iopub.execute_input":"2023-09-01T16:47:20.205111Z","iopub.status.idle":"2023-09-01T19:48:24.290618Z","shell.execute_reply.started":"2023-09-01T16:47:20.205081Z","shell.execute_reply":"2023-09-01T19:48:24.289631Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33b2fe2b5575400c8839c11679398678"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aefe9f9c760a4b79a8f9fe052d400b67"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset opus_books/en-it (download: 3.14 MiB, generated: 8.58 MiB, post-processed: Unknown size, total: 11.72 MiB) to /root/.cache/huggingface/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/3.30M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1458f7685163401ea94ccfc225d648de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/32332 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset opus_books downloaded and prepared to /root/.cache/huggingface/datasets/opus_books/en-it/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\nMax length of source sentence: 309\nMax length of target sentence: 274\nMax length of source sentence: 309\nMax length of target sentence: 274\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: Were you jealous, Jane?\"\n    TARGET: Eravate gelosa, Jane?\n PREDICTED: godono godono godono godono godono godono godono consigliò consigliò consigliò consigliò consigliò consigliò consigliò consigliò consigliò godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono consigliò circostante consigliò godono godono godono godono godono godono godono godono godono godono consigliò circostante consigliò godono godono godono godono godono godono godono godono godono consigliò circostante consigliò circostante consigliò circostante consigliò circostante consigliò circostante metà metà metà metà metà metà metà consigliò godono godono consigliò consigliò cranio metà metà metà metà metà godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono circostante circostante circostante circostante metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà consigliò consigliò consigliò metà consigliò metà godono godono metà metà metà metà godono godono godono godono godono godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà metà consigliò godono godono godono godono godono godono godono godono godono godono godono godono godono consigliò metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono godono godono metà metà metà godono godono godono godono godono metà metà metà metà metà consigliò godono consigliò godono consigliò metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono godono godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono godono metà metà metà metà godono godono godono godono godono metà metà metà godono godono godono godono godono godono godono metà metà metà godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono metà metà metà metà godono godono godono godono godono godono godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà godono godono\n----------\n    SOURCE: Not one of her children had crawled like that.\n    TARGET: Neanche uno dei suoi bambini s’era trascinato così.\n PREDICTED: godono godono godono godono godono godono godono godono godono godono godono godono godono consigliò godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono godono circostante circostante godono godono godono godono godono godono matrimoniale matrimoniale matrimoniale matrimoniale matrimoniale consigliò circostante consigliò funzionari matrimoniale funzionari matrimoniale matrimoniale matrimoniale godono godono godono funzionari cranio funzionari matrimoniale matrimoniale matrimoniale matrimoniale matrimoniale metà matrimoniale matrimoniale matrimoniale matrimoniale matrimoniale matrimoniale metà metà funzionari matrimoniale matrimoniale matrimoniale cranio cranio cranio metà metà metà matrimoniale metà cranio funzionari cranio godono godono godono godono funzionari funzionari funzionari funzionari funzionari metà funzionari metà funzionari matrimoniale matrimoniale matrimoniale metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono godono metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà godono godono godono metà metà metà metà metà funzionari matrimoniale matrimoniale fermatosi metà metà metà metà metà metà matrimoniale matrimoniale matrimoniale matrimoniale metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà fermatosi fermatosi fermatosi fermatosi fermatosi metà metà metà metà fermatosi fermatosi fermatosi metà metà metà metà metà fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi fermatosi godono godono godono godono metà fermatosi fermatosi fermatosi metà godono godono selvatiche metà fermatosi fermatosi metà fermatosi metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà fermatosi fermatosi metà metà metà metà metà metà metà metà metà metà metà metà metà metà metà\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c076810ab3644f48908abc5d883db81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: 'Papa!' exclaimed Kitty and closed his mouth with her hands.\n    TARGET: — Papà — gridò Kitty e gli chiuse la bocca con le mani.\n PREDICTED: — Ma , — disse Levin , e si disse , e si fermò .\n----------\n    SOURCE: \"Thank you: I shall do: I have no broken bones,--only a sprain;\" and again he stood up and tried his foot, but the result extorted an involuntary \"Ugh!\"\n    TARGET: — Grazie, non ho nulla di rotto, si tratta di una storta. Volle provarsi un'altra volta a camminare, ma involontariamente gettò un grido.\n PREDICTED: — Sì , — disse , — mi disse , — mi disse , — e la signora , — e la signora , e la signora , e la signora .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: At the very moment that Vronsky thought it time to pass Makhotin, Frou-Frou, understanding what was in his mind, without any urging, considerably increased her speed and began to draw nearer to Makhotin on the side where it was most advantageous to pass him – the side of the rope, Makhotin would not let her pass that side.\n    TARGET: Proprio nel momento in cui Vronskij pensava di oltrepassare Machotin, Frou-Frou stessa, intuendone il pensiero, senza essere stimolata, accelerò notevolmente il galoppo, e cominciò ad avvicinarsi a Machotin dal lato più conveniente, cioè rasente la corda. Machotin però non lasciava andare la corda.\n PREDICTED: In quel momento , Levin , che era stato di lui , Levin , che , Levin , che , Levin , che , in quel momento , che , in quel momento , in un ’ altra parte , che , in cui , in cui , in un ’ altra volta , che non si era stato in un ’ altra parte di cui si era stato in un ’ altra parte di cui si era stato di un ’ altra parte di cui non si era stato in un ’ altra parte di cui si era stato di cui , e di cui il suo modo di cui si era stato in un ’ altra parte di cui si era stato di un ’ altra parte di cui il suo modo di un ’ altra parte di cui , e , e , e , e , e che , e , e , e di un ’ altra parte di cui il suo momento , e di cui il suo momento , e di cui il suo momento , e che il suo modo di un momento , e di cui si era stato di cui si era stato di cui il suo modo di cui si era stato di cui si era stato di cui era stato di cui , e di cui si era stato di cui , e di cui , e , e , e di cui , e di cui si era stato di cui , e di cui si era stato di cui si era stato di cui si era stato di cui , e , e che , e che , e che , e che , e che , e che , e che , e che , e che , e che , e che , e , e , e che , e , e , e che , e che , e , e , e che , e che , e , e , e , e , e , e , e\n----------\n    SOURCE: It was old and beginning to decay.\n    TARGET: Ma ormai anch’essa era cadente e ammuffita.\n PREDICTED: Era un ’ altra cosa .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: 'You will allow me to listen?' he asked.\n    TARGET: — Mi permettete di ascoltare? — chiese.\n PREDICTED: — Avete bisogno di me ? — domandò .\n----------\n    SOURCE: \"Well?\" I said, as he again paused--\"proceed.\"\n    TARGET: — Bene, — dissi, quando tacque, — continuate.\n PREDICTED: — Ebbene , — disse , — e io sono felice .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: I don't very well know what I did with my hands, but he called me \"Rat!\n    TARGET: Non so dire quello che io facessi con le mani, ma John mi chiamava: \"Talpa!\n PREDICTED: Non so che cosa mi , ma mi misi a piangere , ma mi disse :\n----------\n    SOURCE: So I asked him if he would, and if we might venture over in her. “Yes,” he said, “we venture over in her very well, though great blow wind.”\n    TARGET: Gli chiesi pertanto se voleva e se dovevamo arrischiarci sovr’essa. — «Sì, rispose, potere e volere, anche se soffiar vento grande .»\n PREDICTED: E se fosse andato a me , e se fosse andato a casa , se ne , , , , il fuoco , disse : — « « « , come se ne , come se ne , come un vento di vento .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: He was dressed now: he still looked pale, but he was no longer gory and sullied.\n    TARGET: Era vestito e mi parve debolissimo, ma su di lui non c'era nessuna traccia di sangue.\n PREDICTED: Egli era troppo forte , ma non era più forte , ma non era più bello .\n----------\n    SOURCE: Having lived most of his life in the country and in close contact with the peasants, Levin always felt, at this busy time, that this general stimulation of the peasants communicated itself to him.\n    TARGET: Avendo vissuto la maggior parte della sua vita in campagna e in rapporti intimi con la gente di campagna, Levin, nel periodo di lavoro, sentiva sempre che quella generale eccitazione del popolo si comunicava anche a lui.\n PREDICTED: sempre più la vita di vita e di campagna , Levin si sentiva sempre più forte , che Levin sentiva che , in questo tempo , in questo tempo , era con l ’ azienda dell ’ azienda .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: \"You see now, my queenly Blanche,\" began Lady Ingram, \"she encroaches. Be advised, my angel girl--and--\"\n    TARGET: — Vedete, mia regale Bianca, essa diventa sempre più esigente.\n PREDICTED: — Avete fatto bene , signorina , — continuò Bianca , — venite a mio zio , e mi ha detto :\n----------\n    SOURCE: \"Well, I would rather die yonder than in a street or on a frequented road,\" I reflected. \"And far better that crows and ravens--if any ravens there be in these regions--should pick my flesh from my bones, than that they should be prisoned in a workhouse coffin and moulder in a pauper's grave.\"\n    TARGET: — Ebbene, — dissi a me stessa, — preferisco morir qui piuttosto che su una strada frequentata, e se vi sono corvi nel vicinato preferisco che essi si pascano delle mie carni piuttosto che sapere il mio corpo rinchiuso nella bara di un ospedale e sepolto nella fossa dei poveri.\n PREDICTED: — Ebbene , vorrei esser buona , o in mezzo a un paese o in mezzo a una casa , — mi disse , — e se mi sarei stato meglio che mi in Inghilterra , e se mi sarei stato un di in Inghilterra , e se mi in una .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: Levin turned round.\n    TARGET: Levin si voltò a guardare.\n PREDICTED: Levin si voltò .\n----------\n    SOURCE: Now hear how the deacon will roar, \"Wives, obey your husbands\"!'\n    TARGET: Su, senti come urla il diacono: “che tema suo marito”.\n PREDICTED: Ora , come un sorriso , i vostri diritti , le quali la vostra volontà .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: \"Mr. Wood is in the vestry, sir, putting on his surplice.\"\n    TARGET: — Il signor Wood è giunto e si veste.\n PREDICTED: — Il signor Rochester è in mezzo al sicuro , signore , e nel suo sangue .\n----------\n    SOURCE: 'WELL, HAVE YOU HAD A GOOD TIME?' she asked, coming out to meet him with a meek and repentant look on her face.\n    TARGET: — Be’, c’è stata allegria? — chiese lei, uscendogli incontro con un’espressione colpevole e mansueta nel viso.\n PREDICTED: — Be ’, tu , sei stato fatto da te ? — ella disse , avvicinandosi a lui con un gesto energico , con un gesto spaventato e preoccupato .\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: 'Yes, yes!'\n    TARGET: — Sì, sì.\n PREDICTED: — Sì , sì !\n----------\n    SOURCE: What are you?\n    TARGET: Cosa mai siete?\n PREDICTED: Che cosa avete ?\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"----------\n    SOURCE: [Long argument between Harris and Harris's friend as to what Harris is really singing.\n    TARGET: (Lunga discussione fra Harris e l’amico di Harris su ciò che Harris realmente canti.\n PREDICTED: di Harris e di , come Harris di Harris , che è veramente veramente .\n----------\n    SOURCE: What an appetite I get in the country, wonderful!\n    TARGET: Che appetito m’è venuto in campagna, un prodigio!\n PREDICTED: Che gioia ho fatto in campagna , in campagna !\n","output_type":"stream"}]},{"cell_type":"code","source":"model_filename = \"transformer.pth\"\ntorch.save(\n            {\n                \"epoch\": \"10\",\n                \"model_state_dict\": model.state_dict(),\n            },\n            model_filename,\n)","metadata":{"execution":{"iopub.status.busy":"2023-09-01T19:48:56.910694Z","iopub.execute_input":"2023-09-01T19:48:56.911074Z","iopub.status.idle":"2023-09-01T19:48:57.269767Z","shell.execute_reply.started":"2023-09-01T19:48:56.911045Z","shell.execute_reply":"2023-09-01T19:48:57.268735Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}