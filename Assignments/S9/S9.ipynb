{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"__yvIi5VlXog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687805681271,"user_tz":-330,"elapsed":3062,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"6ef3fc89-d29c-4f68-cfa0-ef6f21fb6016"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/GitHub/ERA-V1/Assignments/S9\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/GitHub/ERA-V1/Assignments/S9"]},{"cell_type":"code","source":["%%writefile src/data_setup.py\n","import os\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(train_dir, test_dir, train_transforms, test_transforms, batch_size, num_workers=NUM_WORKERS):\n","\n","  # Download the dataset\n","  train_data = datasets.CIFAR10(train_dir, train=True, download=True, transform=train_transforms)\n","  test_data = datasets.CIFAR10(test_dir, train=False, download=True, transform=test_transforms)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G62iYYMTlsGp","executionInfo":{"status":"ok","timestamp":1687805681272,"user_tz":-330,"elapsed":11,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"e3536ba4-26d3-4cc9-906e-e3ecacc1d264"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/data_setup.py\n"]}]},{"cell_type":"code","source":["%%writefile src/model_builder.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Model1(nn.Module):\n","    def __init__(self):\n","        super(Model1, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, bias=False),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(32, 32, kernel_size=3, stride=1, bias=False),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False),\n","            nn.ReLU())\n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size = 2, stride = 2))\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, bias=False),\n","            nn.ReLU())\n","        self.layer6 = nn.Sequential(\n","            nn.Conv2d(128, 10, kernel_size=1, stride=1, bias=False))\n","        self.gap = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1))\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = self.layer6(x)\n","        x = self.gap(x)\n","        x = x.view((x.shape[0],-1))\n","        x = F.log_softmax(x, dim=1)\n","\n","        return x\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcVrKeQaoYDW","executionInfo":{"status":"ok","timestamp":1687805681272,"user_tz":-330,"elapsed":9,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"fa36f6f0-a370-4b11-8d63-8b3dd7ee5e2b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/model_builder.py\n"]}]},{"cell_type":"code","source":["%%writefile src/engine.py\n","from src.utils import plot_graph, show_incorrect_images\n","import torch\n","\n","from tqdm.auto import tqdm\n","\n","def GetCorrectPredCount(pPrediction, pLabels):\n","  return pPrediction.argmax(dim=1).eq(pLabels).sum().item()\n","\n","def train_step(model, device, train_loader, optimizer, criterion):\n","  model.train()\n","  pbar = tqdm(train_loader)\n","\n","  train_loss = 0\n","  correct = 0\n","  processed = 0\n","\n","  for batch_idx, (data, target) in enumerate(pbar):\n","    data, target = data.to(device), target.to(device)\n","    optimizer.zero_grad()\n","\n","    # Predict\n","    pred = model(data)\n","\n","    # Calculate loss\n","    loss = criterion(pred, target)\n","    train_loss+=loss.item()\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","\n","    correct += GetCorrectPredCount(pred, target)\n","    processed += len(data)\n","\n","    pbar.set_description(desc= f'Train: Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n","\n","  train_acc = 100*correct/processed\n","  return train_loss, train_acc\n","\n","def test_step(model, device, test_loader, criterion):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","    test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            test_loss += criterion(output, target).item()  # sum up batch loss\n","\n","            pred = output.argmax(dim=1)\n","            correct_mask = pred.eq(target)\n","            incorrect_indices = ~correct_mask\n","\n","            test_incorrect_pred['images'].extend(data[incorrect_indices])\n","            test_incorrect_pred['ground_truths'].extend(target[incorrect_indices])\n","            test_incorrect_pred['predicted_vals'].extend(pred[incorrect_indices])\n","\n","            correct += GetCorrectPredCount(output, target)\n","\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_acc = 100. * correct / len(test_loader.dataset)\n","\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    return test_loss, test_acc, test_incorrect_pred\n","\n","def train(model, train_loader, test_loader, device, optimizer, epochs, criterion):\n","\n","    class_map = {\n","        0: 'airplane',\n","        1: 'automobile',\n","        2: 'bird',\n","        3: 'cat',\n","        4: 'deer',\n","        5: 'dog',\n","        6: 'frog',\n","        7: 'horse',\n","        8: 'ship',\n","        9: 'truck'\n","    }\n","    # Data to plot accuracy and loss graphs\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","        \"train_acc\": [],\n","        \"test_loss\": [],\n","        \"test_acc\": []\n","    }\n","\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch}')\n","        train_loss, train_acc = train_step(model=model, device=device, train_loader=train_loader, optimizer=optimizer, criterion=criterion)\n","        test_loss, test_acc, test_incorrect_pred = test_step(model=model, device=device, test_loader=test_loader, criterion=criterion)\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    plot_graph(results[\"train_loss\"], results[\"test_loss\"], results[\"train_acc\"], results[\"test_acc\"])\n","    show_incorrect_images(test_incorrect_pred, class_map)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dVAwtcSomqr","executionInfo":{"status":"ok","timestamp":1687805681272,"user_tz":-330,"elapsed":7,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"852d94d4-4b8a-4350-b0ea-77255dca19e8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/engine.py\n"]}]},{"cell_type":"code","source":["%%writefile src/utils.py\n","import torch\n","from pathlib import Path\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def save_model(model, target_dir, model_name):\n","  \"\"\"Saves a PyTorch model to a target directory.\n","\n","  Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model. Should include\n","      either \".pth\" or \".pt\" as the file extension.\n","\n","  Example usage:\n","    save_model(model=model_0,\n","               target_dir=\"models\",\n","               model_name=\"05_going_modular_tingvgg_model.pth\")\n","  \"\"\"\n","  # Create target directory\n","  target_dir_path = Path(target_dir)\n","  target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","\n","  # Create model save path\n","  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","  model_save_path = target_dir_path / model_name\n","\n","  # Save the model state_dict()\n","  print(f\"[INFO] Saving model to: {model_save_path}\")\n","  torch.save(obj=model.state_dict(),\n","             f=model_save_path)\n","\n","def plot_graph(train_losses, test_losses, train_acc, test_acc):\n","    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n","\n","    # Plot Train and Test Loss\n","    axs[0].plot(train_losses, label='Train Loss')\n","    axs[0].plot(test_losses, label='Test Loss')\n","    axs[0].set_title(\"Train and Test Loss\")\n","    axs[0].set_xlabel(\"Epoch\")\n","    axs[0].set_ylabel(\"Loss\")\n","    axs[0].legend()\n","\n","    # Plot Train and Test Accuracy\n","    axs[1].plot(train_acc, label='Train Accuracy')\n","    axs[1].plot(test_acc, label='Test Accuracy')\n","    axs[1].set_title(\"Train and Test Accuracy\")\n","    axs[1].set_xlabel(\"Epoch\")\n","    axs[1].set_ylabel(\"Accuracy\")\n","    axs[1].legend()\n","\n","    plt.savefig(\"models/loss_accuracy_plot.png\")\n","\n","def show_incorrect_images(test_incorrect_pred, class_map):\n","    num_images = 10\n","    num_rows = 2\n","    num_cols = (num_images + 1) // 2  # Adjust the number of columns based on the number of images\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n","\n","    for i in range(num_images):\n","        row_idx = i // num_cols\n","        col_idx = i % num_cols\n","\n","        img = test_incorrect_pred['images'][i].cpu().numpy()\n","        img = np.transpose(img, (1, 2, 0))\n","        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalize the image data\n","        label = test_incorrect_pred['ground_truths'][i].cpu().item()\n","        pred = test_incorrect_pred['predicted_vals'][i].cpu().item()\n","\n","        axs[row_idx, col_idx].imshow(img)\n","        axs[row_idx, col_idx].set_title(f'GT: {class_map[label]}, Pred: {class_map[pred]}')\n","        axs[row_idx, col_idx].axis('off')\n","\n","    plt.savefig(\"models/incorrect_images.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuSB17JWrg9H","executionInfo":{"status":"ok","timestamp":1687805681273,"user_tz":-330,"elapsed":6,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"92585569-000b-4318-ca52-9511bd4746a2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/utils.py\n"]}]},{"cell_type":"code","source":["%%writefile train.py\n","import os\n","import torch\n","from src import data_setup, engine, model_builder, utils\n","\n","from torchvision import transforms\n","\n","# Setup hyperparameters\n","NUM_EPOCHS = 2\n","BATCH_SIZE = 32\n","HIDDEN_UNITS = 10\n","LEARNING_RATE = 0.001\n","MOMENTUM = 0.9\n","\n","# Setup directories\n","train_dir = \"../data\"\n","test_dir = \"../data\"\n","\n","# Setup target device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create transforms\n","# Train Phase transformations\n","train_transforms = transforms.Compose([\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n","                                       ])\n","\n","# Test Phase transformations\n","test_transforms = transforms.Compose([\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n","                                       ])\n","\n","# Create DataLoaders with help from data_setup.py\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    train_transforms=train_transforms,\n","    test_transforms=test_transforms,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Create model with help from model_builder.py\n","model = model_builder.Model1().to(device)\n","\n","# Set loss and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n","\n","\n","# Start training with help from engine.py\n","engine.train(model=model,\n","             train_loader=train_dataloader,\n","             test_loader=test_dataloader,\n","             criterion=criterion,\n","             optimizer=optimizer,\n","             epochs=NUM_EPOCHS,\n","             device=device)\n","\n","# Save the model with help from utils.py\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BALOYpMrrkby","executionInfo":{"status":"ok","timestamp":1687805681273,"user_tz":-330,"elapsed":5,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"afd5b737-e15c-47ec-ae25-b1600a596459"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train.py\n"]}]},{"cell_type":"code","source":["!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjAe7zfYtfJB","executionInfo":{"status":"ok","timestamp":1687805747371,"user_tz":-330,"elapsed":66102,"user":{"displayName":"SELVARAJ S","userId":"10860943232956077709"}},"outputId":"eae95881-aa08-41f0-ebd1-c5992a87c2c9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Epoch 0\n","Train: Loss=1.8658 Batch_id=1562 Accuracy=14.23: 100% 1563/1563 [00:24<00:00, 63.29it/s]\n","Test set: Average loss: 0.0637, Accuracy: 2407/10000 (24.07%)\n","\n","Epoch 1\n","Train: Loss=1.5459 Batch_id=1562 Accuracy=30.02: 100% 1563/1563 [00:20<00:00, 76.01it/s]\n","Test set: Average loss: 0.0545, Accuracy: 3605/10000 (36.05%)\n","\n","[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"]}]},{"cell_type":"code","source":["!git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKe5hKkl-Jwf","outputId":"8489b5fc-0ca3-4ae5-cf40-0141015923e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Refresh index: 100% (27/27), done.\n"]}]},{"cell_type":"code","source":["!"],"metadata":{"id":"eLvDMJOqEXxS"},"execution_count":null,"outputs":[]}]}