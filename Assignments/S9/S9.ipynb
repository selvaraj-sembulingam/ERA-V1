{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"__yvIi5VlXog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687894183877,"user_tz":-330,"elapsed":3672,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"513f60f6-cda7-4f04-9366-4cc52a4c6721"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/GitHub/ERA-V1/Assignments/S9\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/GitHub/ERA-V1/Assignments/S9"]},{"cell_type":"code","source":["%%writefile src/data_setup.py\n","import os\n","import numpy as np\n","\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","\n","\n","NUM_WORKERS = os.cpu_count()\n","\n","class AlbumentationsDataset(Dataset):\n","    def __init__(self, dataset, transforms):\n","        self.dataset = dataset\n","        self.transforms = transforms\n","        self.classes = dataset.classes\n","\n","    def __getitem__(self, index):\n","        image, target = self.dataset[index]\n","\n","        image = np.array(image)\n","\n","        # Apply Albumentations transforms\n","        transformed = self.transforms(image=image)\n","        image = transformed['image']\n","\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","def create_dataloaders(train_dir, test_dir, train_transforms, test_transforms, batch_size, num_workers=NUM_WORKERS):\n","\n","  # Download the dataset\n","  train_data = datasets.CIFAR10(train_dir, train=True, download=True)\n","  test_data = datasets.CIFAR10(test_dir, train=False, download=True)\n","\n","  # Wrap the datasets with AlbumentationsDataset\n","  train_data = AlbumentationsDataset(train_data, train_transforms)\n","  test_data = AlbumentationsDataset(test_data, test_transforms)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G62iYYMTlsGp","executionInfo":{"status":"ok","timestamp":1687894183878,"user_tz":-330,"elapsed":8,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"955abc98-78a0-4d0e-93fd-6991b9214611"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/data_setup.py\n"]}]},{"cell_type":"code","source":["%%writefile src/model_builder.py\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Model1(nn.Module):\n","    def __init__(self, dropout_value=0.01):\n","        super(Model1, self).__init__()\n","        self.block1 = nn.Sequential(\n","            nn.Conv2d(3, 8, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(8, 8, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(8, 8, kernel_size=3, stride=2, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(8),\n","            nn.Dropout(dropout_value))\n","        self.block2 = nn.Sequential(\n","            nn.Conv2d(8, 16, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(16, 16, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(16, 16, kernel_size=3, stride=2, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(16),\n","            nn.Dropout(dropout_value))\n","        self.block3 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(32, 32, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(32, 32, kernel_size=3, stride=2, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(32),\n","            nn.Dropout(dropout_value))\n","        self.block4 = nn.Sequential(\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, bias=False, padding=1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(64),\n","            nn.Dropout(dropout_value))\n","        self.gap = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1))\n","        self.block5 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=1, stride=1, bias=False),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(128),\n","            nn.Dropout(dropout_value),\n","            nn.Conv2d(128, 10, kernel_size=1, stride=1, bias=False))\n","\n","    def forward(self, x):\n","        x = self.block1(x)\n","        x = self.block2(x)\n","        x = self.block3(x)\n","        x = self.block4(x)\n","        x = self.gap(x)\n","        x = self.block5(x)\n","        x = x.view((x.shape[0],-1))\n","        x = F.log_softmax(x, dim=1)\n","        return x\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcVrKeQaoYDW","executionInfo":{"status":"ok","timestamp":1687894183878,"user_tz":-330,"elapsed":6,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"5646e95c-b9d7-4907-972c-e2b26826d835"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/model_builder.py\n"]}]},{"cell_type":"code","source":["!pip install torchsummary\n","import torch\n","from src.model_builder import Model1 as Net\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","print(device)\n","model = Net().to(device)\n","summary(model, input_size=(3, 32, 32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2P2w1BNj-dK","executionInfo":{"status":"ok","timestamp":1687894198969,"user_tz":-330,"elapsed":15096,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"5de4f092-69f3-43ab-d47a-48c033b74cd2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.10/dist-packages (1.5.1)\n","cuda\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 32, 32]             216\n","              ReLU-2            [-1, 8, 32, 32]               0\n","       BatchNorm2d-3            [-1, 8, 32, 32]              16\n","           Dropout-4            [-1, 8, 32, 32]               0\n","            Conv2d-5            [-1, 8, 32, 32]             576\n","              ReLU-6            [-1, 8, 32, 32]               0\n","       BatchNorm2d-7            [-1, 8, 32, 32]              16\n","           Dropout-8            [-1, 8, 32, 32]               0\n","            Conv2d-9            [-1, 8, 16, 16]             576\n","             ReLU-10            [-1, 8, 16, 16]               0\n","      BatchNorm2d-11            [-1, 8, 16, 16]              16\n","          Dropout-12            [-1, 8, 16, 16]               0\n","           Conv2d-13           [-1, 16, 16, 16]           1,152\n","             ReLU-14           [-1, 16, 16, 16]               0\n","      BatchNorm2d-15           [-1, 16, 16, 16]              32\n","          Dropout-16           [-1, 16, 16, 16]               0\n","           Conv2d-17           [-1, 16, 16, 16]           2,304\n","             ReLU-18           [-1, 16, 16, 16]               0\n","      BatchNorm2d-19           [-1, 16, 16, 16]              32\n","          Dropout-20           [-1, 16, 16, 16]               0\n","           Conv2d-21             [-1, 16, 8, 8]           2,304\n","             ReLU-22             [-1, 16, 8, 8]               0\n","      BatchNorm2d-23             [-1, 16, 8, 8]              32\n","          Dropout-24             [-1, 16, 8, 8]               0\n","           Conv2d-25             [-1, 32, 8, 8]           4,608\n","             ReLU-26             [-1, 32, 8, 8]               0\n","      BatchNorm2d-27             [-1, 32, 8, 8]              64\n","          Dropout-28             [-1, 32, 8, 8]               0\n","           Conv2d-29             [-1, 32, 8, 8]           9,216\n","             ReLU-30             [-1, 32, 8, 8]               0\n","      BatchNorm2d-31             [-1, 32, 8, 8]              64\n","          Dropout-32             [-1, 32, 8, 8]               0\n","           Conv2d-33             [-1, 32, 4, 4]           9,216\n","             ReLU-34             [-1, 32, 4, 4]               0\n","      BatchNorm2d-35             [-1, 32, 4, 4]              64\n","          Dropout-36             [-1, 32, 4, 4]               0\n","           Conv2d-37             [-1, 64, 4, 4]          18,432\n","             ReLU-38             [-1, 64, 4, 4]               0\n","      BatchNorm2d-39             [-1, 64, 4, 4]             128\n","          Dropout-40             [-1, 64, 4, 4]               0\n","           Conv2d-41             [-1, 64, 4, 4]          36,864\n","             ReLU-42             [-1, 64, 4, 4]               0\n","      BatchNorm2d-43             [-1, 64, 4, 4]             128\n","          Dropout-44             [-1, 64, 4, 4]               0\n","           Conv2d-45             [-1, 64, 4, 4]          36,864\n","             ReLU-46             [-1, 64, 4, 4]               0\n","      BatchNorm2d-47             [-1, 64, 4, 4]             128\n","          Dropout-48             [-1, 64, 4, 4]               0\n","AdaptiveAvgPool2d-49             [-1, 64, 1, 1]               0\n","           Conv2d-50            [-1, 128, 1, 1]           8,192\n","             ReLU-51            [-1, 128, 1, 1]               0\n","      BatchNorm2d-52            [-1, 128, 1, 1]             256\n","          Dropout-53            [-1, 128, 1, 1]               0\n","           Conv2d-54             [-1, 10, 1, 1]           1,280\n","================================================================\n","Total params: 132,776\n","Trainable params: 132,776\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.08\n","Params size (MB): 0.51\n","Estimated Total Size (MB): 1.60\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["%%writefile src/engine.py\n","from src.utils import plot_graph, show_incorrect_images\n","import torch\n","\n","from tqdm.auto import tqdm\n","\n","def GetCorrectPredCount(pPrediction, pLabels):\n","  return pPrediction.argmax(dim=1).eq(pLabels).sum().item()\n","\n","def train_step(model, device, train_loader, optimizer, criterion):\n","  model.train()\n","  pbar = tqdm(train_loader)\n","\n","  train_loss = 0\n","  correct = 0\n","  processed = 0\n","\n","  for batch_idx, (data, target) in enumerate(pbar):\n","    data, target = data.to(device), target.to(device)\n","    optimizer.zero_grad()\n","\n","    # Predict\n","    pred = model(data)\n","\n","    # Calculate loss\n","    loss = criterion(pred, target)\n","    train_loss+=loss.item()\n","\n","    # Backpropagation\n","    loss.backward()\n","    optimizer.step()\n","\n","    correct += GetCorrectPredCount(pred, target)\n","    processed += len(data)\n","\n","    pbar.set_description(desc= f'Train: Loss={loss.item():0.4f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n","\n","  train_acc = 100*correct/processed\n","  return train_loss, train_acc\n","\n","def test_step(model, device, test_loader, criterion):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","    test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}\n","\n","    with torch.no_grad():\n","        for batch_idx, (data, target) in enumerate(test_loader):\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","            test_loss += criterion(output, target).item()  # sum up batch loss\n","\n","            pred = output.argmax(dim=1)\n","            correct_mask = pred.eq(target)\n","            incorrect_indices = ~correct_mask\n","\n","            test_incorrect_pred['images'].extend(data[incorrect_indices])\n","            test_incorrect_pred['ground_truths'].extend(target[incorrect_indices])\n","            test_incorrect_pred['predicted_vals'].extend(pred[incorrect_indices])\n","\n","            correct += GetCorrectPredCount(output, target)\n","\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_acc = 100. * correct / len(test_loader.dataset)\n","\n","\n","    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","    return test_loss, test_acc, test_incorrect_pred\n","\n","def train(model, train_loader, test_loader, device, optimizer, epochs, criterion, scheduler):\n","\n","    class_map = {\n","        0: 'airplane',\n","        1: 'automobile',\n","        2: 'bird',\n","        3: 'cat',\n","        4: 'deer',\n","        5: 'dog',\n","        6: 'frog',\n","        7: 'horse',\n","        8: 'ship',\n","        9: 'truck'\n","    }\n","    # Data to plot accuracy and loss graphs\n","    # Create empty results dictionary\n","    results = {\"train_loss\": [],\n","        \"train_acc\": [],\n","        \"test_loss\": [],\n","        \"test_acc\": []\n","    }\n","\n","    for epoch in range(epochs):\n","        print(f'Epoch {epoch}')\n","        train_loss, train_acc = train_step(model=model, device=device, train_loader=train_loader, optimizer=optimizer, criterion=criterion)\n","        test_loss, test_acc, test_incorrect_pred = test_step(model=model, device=device, test_loader=test_loader, criterion=criterion)\n","        scheduler.step()\n","\n","        # Update results dictionary\n","        results[\"train_loss\"].append(train_loss)\n","        results[\"train_acc\"].append(train_acc)\n","        results[\"test_loss\"].append(test_loss)\n","        results[\"test_acc\"].append(test_acc)\n","\n","    plot_graph(results[\"train_loss\"], results[\"test_loss\"], results[\"train_acc\"], results[\"test_acc\"])\n","    show_incorrect_images(test_incorrect_pred, class_map)\n","\n","    # Return the filled results at the end of the epochs\n","    return results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dVAwtcSomqr","executionInfo":{"status":"ok","timestamp":1687894198969,"user_tz":-330,"elapsed":17,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"6dcfc8da-e314-4db1-d03d-0dfff91ec60b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/engine.py\n"]}]},{"cell_type":"code","source":["%%writefile src/utils.py\n","import torch\n","from pathlib import Path\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def save_model(model, target_dir, model_name):\n","  \"\"\"Saves a PyTorch model to a target directory.\n","\n","  Args:\n","    model: A target PyTorch model to save.\n","    target_dir: A directory for saving the model to.\n","    model_name: A filename for the saved model. Should include\n","      either \".pth\" or \".pt\" as the file extension.\n","\n","  Example usage:\n","    save_model(model=model_0,\n","               target_dir=\"models\",\n","               model_name=\"05_going_modular_tingvgg_model.pth\")\n","  \"\"\"\n","  # Create target directory\n","  target_dir_path = Path(target_dir)\n","  target_dir_path.mkdir(parents=True,\n","                        exist_ok=True)\n","\n","  # Create model save path\n","  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n","  model_save_path = target_dir_path / model_name\n","\n","  # Save the model state_dict()\n","  print(f\"[INFO] Saving model to: {model_save_path}\")\n","  torch.save(obj=model.state_dict(),\n","             f=model_save_path)\n","\n","def plot_graph(train_losses, test_losses, train_acc, test_acc):\n","    fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n","\n","    # Plot Train and Test Loss\n","    axs[0].plot(train_losses, label='Train Loss')\n","    axs[0].plot(test_losses, label='Test Loss')\n","    axs[0].set_title(\"Train and Test Loss\")\n","    axs[0].set_xlabel(\"Epoch\")\n","    axs[0].set_ylabel(\"Loss\")\n","    axs[0].legend()\n","\n","    # Plot Train and Test Accuracy\n","    axs[1].plot(train_acc, label='Train Accuracy')\n","    axs[1].plot(test_acc, label='Test Accuracy')\n","    axs[1].set_title(\"Train and Test Accuracy\")\n","    axs[1].set_xlabel(\"Epoch\")\n","    axs[1].set_ylabel(\"Accuracy\")\n","    axs[1].legend()\n","\n","    plt.savefig(\"models/loss_accuracy_plot.png\")\n","\n","def show_incorrect_images(test_incorrect_pred, class_map):\n","    num_images = 10\n","    num_rows = 2\n","    num_cols = (num_images + 1) // 2  # Adjust the number of columns based on the number of images\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(num_cols * 2, num_rows * 2))\n","\n","    for i in range(num_images):\n","        row_idx = i // num_cols\n","        col_idx = i % num_cols\n","\n","        img = test_incorrect_pred['images'][i].cpu().numpy()\n","        img = np.transpose(img, (1, 2, 0))\n","        img = (img - np.min(img)) / (np.max(img) - np.min(img))  # Normalize the image data\n","        label = test_incorrect_pred['ground_truths'][i].cpu().item()\n","        pred = test_incorrect_pred['predicted_vals'][i].cpu().item()\n","\n","        axs[row_idx, col_idx].imshow(img)\n","        axs[row_idx, col_idx].set_title(f'GT: {class_map[label]}, Pred: {class_map[pred]}')\n","        axs[row_idx, col_idx].axis('off')\n","\n","    plt.savefig(\"models/incorrect_images.png\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VuSB17JWrg9H","executionInfo":{"status":"ok","timestamp":1687894198970,"user_tz":-330,"elapsed":17,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"d3672d69-a91c-4e74-829d-b700bbe3c3ce"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting src/utils.py\n"]}]},{"cell_type":"code","source":["%%writefile train.py\n","import os\n","import torch\n","from src import data_setup, engine, model_builder, utils\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from torch.optim.lr_scheduler import OneCycleLR\n","\n","from torchvision import transforms\n","\n","# Setup hyperparameters\n","NUM_EPOCHS = 50\n","BATCH_SIZE = 128\n","LEARNING_RATE = 0.001\n","MOMENTUM = 0.9\n","\n","# Setup directories\n","train_dir = \"../data\"\n","test_dir = \"../data\"\n","\n","# Setup target device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Create transforms\n","# Train Phase transformations\n","train_transforms = A.Compose([\n","    A.HorizontalFlip(p=1),  # Apply horizontal flip with probability 1 (always)\n","    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=10, p=0.5),  # Apply shift, scale, and rotation\n","    A.CoarseDropout(max_holes=1, max_height=16, max_width=16, min_holes=1, min_height=16, min_width=16, fill_value=(0.49139968, 0.48215827, 0.44653124), mask_fill_value=None),  # Apply coarse dropout\n","    A.Normalize(mean=[0.49139968, 0.48215827, 0.44653124], std=[0.24703233, 0.24348505, 0.26158768]),  # Normalize the image\n","    ToTensorV2() # Convert image to a PyTorch tensor\n","])\n","\n","\n","# Test Phase transformations\n","test_transforms = A.Compose([\n","    A.Normalize(mean=[0.49139968, 0.48215827, 0.44653124], std=[0.24703233, 0.24348505, 0.26158768]),  # Normalize the image\n","    ToTensorV2()  # Convert image to a PyTorch tensor\n","])\n","\n","\n","# Create DataLoaders with help from data_setup.py\n","train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n","    train_dir=train_dir,\n","    test_dir=test_dir,\n","    train_transforms=train_transforms,\n","    test_transforms=test_transforms,\n","    batch_size=BATCH_SIZE\n",")\n","\n","# Create model with help from model_builder.py\n","model = model_builder.Model1().to(device)\n","\n","# Set loss and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=1e-4)\n","scheduler = OneCycleLR(optimizer, max_lr=0.1, total_steps=50, verbose=True)\n","\n","\n","# Start training with help from engine.py\n","engine.train(model=model,\n","             train_loader=train_dataloader,\n","             test_loader=test_dataloader,\n","             criterion=criterion,\n","             optimizer=optimizer,\n","             epochs=NUM_EPOCHS,\n","             device=device,\n","             scheduler=scheduler)\n","\n","# Save the model with help from utils.py\n","utils.save_model(model=model,\n","                 target_dir=\"models\",\n","                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BALOYpMrrkby","executionInfo":{"status":"ok","timestamp":1687894198970,"user_tz":-330,"elapsed":15,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"c2648412-3430-40df-f4c0-5abb89974493"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting train.py\n"]}]},{"cell_type":"code","source":["!pip install albumentations\n","!python train.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjAe7zfYtfJB","outputId":"037c8597-5e3d-4074-d7d3-9c49552b6c31","executionInfo":{"status":"ok","timestamp":1687895192122,"user_tz":-330,"elapsed":993166,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.2.1)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.10.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.19.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.7.0.72)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations) (4.6.3)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (3.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2.25.1)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.4.12)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations) (23.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Adjusting learning rate of group 0 to 4.0000e-03.\n","Epoch 0\n","Train: Loss=1.7489 Batch_id=390 Accuracy=30.74: 100% 391/391 [00:19<00:00, 19.57it/s]\n","Test set: Average loss: 0.0131, Accuracy: 3964/10000 (39.64%)\n","\n","Adjusting learning rate of group 0 to 5.2035e-03.\n","Epoch 1\n","Train: Loss=1.6458 Batch_id=390 Accuracy=39.73: 100% 391/391 [00:18<00:00, 21.54it/s]\n","Test set: Average loss: 0.0122, Accuracy: 4396/10000 (43.96%)\n","\n","Adjusting learning rate of group 0 to 8.7535e-03.\n","Epoch 2\n","Train: Loss=1.5414 Batch_id=390 Accuracy=43.12: 100% 391/391 [00:17<00:00, 22.47it/s]\n","Test set: Average loss: 0.0108, Accuracy: 5001/10000 (50.01%)\n","\n","Adjusting learning rate of group 0 to 1.4472e-02.\n","Epoch 3\n","Train: Loss=1.4537 Batch_id=390 Accuracy=46.67: 100% 391/391 [00:18<00:00, 21.64it/s]\n","Test set: Average loss: 0.0101, Accuracy: 5434/10000 (54.34%)\n","\n","Adjusting learning rate of group 0 to 2.2072e-02.\n","Epoch 4\n","Train: Loss=1.3568 Batch_id=390 Accuracy=49.81: 100% 391/391 [00:16<00:00, 23.39it/s]\n","Test set: Average loss: 0.0097, Accuracy: 5560/10000 (55.60%)\n","\n","Adjusting learning rate of group 0 to 3.1174e-02.\n","Epoch 5\n","Train: Loss=1.3127 Batch_id=390 Accuracy=53.61: 100% 391/391 [00:16<00:00, 23.67it/s]\n","Test set: Average loss: 0.0085, Accuracy: 6215/10000 (62.15%)\n","\n","Adjusting learning rate of group 0 to 4.1319e-02.\n","Epoch 6\n","Train: Loss=1.4598 Batch_id=390 Accuracy=57.09: 100% 391/391 [00:17<00:00, 22.73it/s]\n","Test set: Average loss: 0.0078, Accuracy: 6483/10000 (64.83%)\n","\n","Adjusting learning rate of group 0 to 5.2000e-02.\n","Epoch 7\n","Train: Loss=1.2778 Batch_id=390 Accuracy=59.25: 100% 391/391 [00:16<00:00, 23.59it/s]\n","Test set: Average loss: 0.0077, Accuracy: 6622/10000 (66.22%)\n","\n","Adjusting learning rate of group 0 to 6.2681e-02.\n","Epoch 8\n","Train: Loss=1.2650 Batch_id=390 Accuracy=60.93: 100% 391/391 [00:16<00:00, 23.52it/s]\n","Test set: Average loss: 0.0073, Accuracy: 6723/10000 (67.23%)\n","\n","Adjusting learning rate of group 0 to 7.2826e-02.\n","Epoch 9\n","Train: Loss=1.0370 Batch_id=390 Accuracy=63.17: 100% 391/391 [00:17<00:00, 22.02it/s]\n","Test set: Average loss: 0.0072, Accuracy: 6808/10000 (68.08%)\n","\n","Adjusting learning rate of group 0 to 8.1928e-02.\n","Epoch 10\n","Train: Loss=1.0996 Batch_id=390 Accuracy=64.26: 100% 391/391 [00:17<00:00, 22.81it/s]\n","Test set: Average loss: 0.0065, Accuracy: 7098/10000 (70.98%)\n","\n","Adjusting learning rate of group 0 to 8.9528e-02.\n","Epoch 11\n","Train: Loss=0.9225 Batch_id=390 Accuracy=65.60: 100% 391/391 [00:17<00:00, 22.96it/s]\n","Test set: Average loss: 0.0065, Accuracy: 7123/10000 (71.23%)\n","\n","Adjusting learning rate of group 0 to 9.5247e-02.\n","Epoch 12\n","Train: Loss=0.9024 Batch_id=390 Accuracy=66.13: 100% 391/391 [00:18<00:00, 21.40it/s]\n","Test set: Average loss: 0.0063, Accuracy: 7224/10000 (72.24%)\n","\n","Adjusting learning rate of group 0 to 9.8797e-02.\n","Epoch 13\n","Train: Loss=0.7423 Batch_id=390 Accuracy=67.21: 100% 391/391 [00:16<00:00, 23.07it/s]\n","Test set: Average loss: 0.0062, Accuracy: 7269/10000 (72.69%)\n","\n","Adjusting learning rate of group 0 to 1.0000e-01.\n","Epoch 14\n","Train: Loss=0.8410 Batch_id=390 Accuracy=68.00: 100% 391/391 [00:16<00:00, 23.30it/s]\n","Test set: Average loss: 0.0062, Accuracy: 7296/10000 (72.96%)\n","\n","Adjusting learning rate of group 0 to 9.9799e-02.\n","Epoch 15\n","Train: Loss=0.7302 Batch_id=390 Accuracy=68.40: 100% 391/391 [00:17<00:00, 22.12it/s]\n","Test set: Average loss: 0.0058, Accuracy: 7442/10000 (74.42%)\n","\n","Adjusting learning rate of group 0 to 9.9196e-02.\n","Epoch 16\n","Train: Loss=1.2176 Batch_id=390 Accuracy=68.77: 100% 391/391 [00:16<00:00, 23.48it/s]\n","Test set: Average loss: 0.0058, Accuracy: 7433/10000 (74.33%)\n","\n","Adjusting learning rate of group 0 to 9.8198e-02.\n","Epoch 17\n","Train: Loss=0.6237 Batch_id=390 Accuracy=69.61: 100% 391/391 [00:16<00:00, 23.32it/s]\n","Test set: Average loss: 0.0058, Accuracy: 7418/10000 (74.18%)\n","\n","Adjusting learning rate of group 0 to 9.6812e-02.\n","Epoch 18\n","Train: Loss=0.7007 Batch_id=390 Accuracy=70.09: 100% 391/391 [00:17<00:00, 22.36it/s]\n","Test set: Average loss: 0.0059, Accuracy: 7478/10000 (74.78%)\n","\n","Adjusting learning rate of group 0 to 9.5048e-02.\n","Epoch 19\n","Train: Loss=1.0241 Batch_id=390 Accuracy=70.32: 100% 391/391 [00:17<00:00, 22.98it/s]\n","Test set: Average loss: 0.0055, Accuracy: 7575/10000 (75.75%)\n","\n","Adjusting learning rate of group 0 to 9.2922e-02.\n","Epoch 20\n","Train: Loss=0.8787 Batch_id=390 Accuracy=70.55: 100% 391/391 [00:16<00:00, 23.18it/s]\n","Test set: Average loss: 0.0055, Accuracy: 7597/10000 (75.97%)\n","\n","Adjusting learning rate of group 0 to 9.0451e-02.\n","Epoch 21\n","Train: Loss=0.8323 Batch_id=390 Accuracy=71.03: 100% 391/391 [00:17<00:00, 22.69it/s]\n","Test set: Average loss: 0.0059, Accuracy: 7424/10000 (74.24%)\n","\n","Adjusting learning rate of group 0 to 8.7654e-02.\n","Epoch 22\n","Train: Loss=0.9339 Batch_id=390 Accuracy=71.56: 100% 391/391 [00:18<00:00, 20.99it/s]\n","Test set: Average loss: 0.0054, Accuracy: 7575/10000 (75.75%)\n","\n","Adjusting learning rate of group 0 to 8.4553e-02.\n","Epoch 23\n","Train: Loss=0.7203 Batch_id=390 Accuracy=71.60: 100% 391/391 [00:18<00:00, 21.52it/s]\n","Test set: Average loss: 0.0056, Accuracy: 7508/10000 (75.08%)\n","\n","Adjusting learning rate of group 0 to 8.1175e-02.\n","Epoch 24\n","Train: Loss=0.6878 Batch_id=390 Accuracy=72.13: 100% 391/391 [00:16<00:00, 23.61it/s]\n","Test set: Average loss: 0.0053, Accuracy: 7693/10000 (76.93%)\n","\n","Adjusting learning rate of group 0 to 7.7545e-02.\n","Epoch 25\n","Train: Loss=0.9798 Batch_id=390 Accuracy=72.61: 100% 391/391 [00:16<00:00, 23.64it/s]\n","Test set: Average loss: 0.0058, Accuracy: 7532/10000 (75.32%)\n","\n","Adjusting learning rate of group 0 to 7.3694e-02.\n","Epoch 26\n","Train: Loss=0.7642 Batch_id=390 Accuracy=72.81: 100% 391/391 [00:17<00:00, 22.72it/s]\n","Test set: Average loss: 0.0051, Accuracy: 7758/10000 (77.58%)\n","\n","Adjusting learning rate of group 0 to 6.9651e-02.\n","Epoch 27\n","Train: Loss=0.8084 Batch_id=390 Accuracy=72.82: 100% 391/391 [00:16<00:00, 23.32it/s]\n","Test set: Average loss: 0.0055, Accuracy: 7620/10000 (76.20%)\n","\n","Adjusting learning rate of group 0 to 6.5451e-02.\n","Epoch 28\n","Train: Loss=0.7032 Batch_id=390 Accuracy=73.36: 100% 391/391 [00:16<00:00, 23.78it/s]\n","Test set: Average loss: 0.0050, Accuracy: 7824/10000 (78.24%)\n","\n","Adjusting learning rate of group 0 to 6.1126e-02.\n","Epoch 29\n","Train: Loss=0.8431 Batch_id=390 Accuracy=73.70: 100% 391/391 [00:16<00:00, 23.20it/s]\n","Test set: Average loss: 0.0050, Accuracy: 7784/10000 (77.84%)\n","\n","Adjusting learning rate of group 0 to 5.6712e-02.\n","Epoch 30\n","Train: Loss=0.5869 Batch_id=390 Accuracy=73.78: 100% 391/391 [00:16<00:00, 23.93it/s]\n","Test set: Average loss: 0.0050, Accuracy: 7854/10000 (78.54%)\n","\n","Adjusting learning rate of group 0 to 5.2243e-02.\n","Epoch 31\n","Train: Loss=0.7464 Batch_id=390 Accuracy=74.30: 100% 391/391 [00:16<00:00, 23.31it/s]\n","Test set: Average loss: 0.0052, Accuracy: 7794/10000 (77.94%)\n","\n","Adjusting learning rate of group 0 to 4.7757e-02.\n","Epoch 32\n","Train: Loss=0.7247 Batch_id=390 Accuracy=74.43: 100% 391/391 [00:17<00:00, 22.75it/s]\n","Test set: Average loss: 0.0049, Accuracy: 7873/10000 (78.73%)\n","\n","Adjusting learning rate of group 0 to 4.3289e-02.\n","Epoch 33\n","Train: Loss=0.7475 Batch_id=390 Accuracy=74.83: 100% 391/391 [00:18<00:00, 21.08it/s]\n","Test set: Average loss: 0.0049, Accuracy: 7893/10000 (78.93%)\n","\n","Adjusting learning rate of group 0 to 3.8874e-02.\n","Epoch 34\n","Train: Loss=0.7625 Batch_id=390 Accuracy=75.17: 100% 391/391 [00:16<00:00, 23.06it/s]\n","Test set: Average loss: 0.0047, Accuracy: 7947/10000 (79.47%)\n","\n","Adjusting learning rate of group 0 to 3.4549e-02.\n","Epoch 35\n","Train: Loss=0.6966 Batch_id=390 Accuracy=75.74: 100% 391/391 [00:16<00:00, 23.27it/s]\n","Test set: Average loss: 0.0047, Accuracy: 7952/10000 (79.52%)\n","\n","Adjusting learning rate of group 0 to 3.0349e-02.\n","Epoch 36\n","Train: Loss=0.9058 Batch_id=390 Accuracy=75.84: 100% 391/391 [00:16<00:00, 23.98it/s]\n","Test set: Average loss: 0.0048, Accuracy: 7951/10000 (79.51%)\n","\n","Adjusting learning rate of group 0 to 2.6307e-02.\n","Epoch 37\n","Train: Loss=0.7683 Batch_id=390 Accuracy=76.46: 100% 391/391 [00:16<00:00, 23.83it/s]\n","Test set: Average loss: 0.0047, Accuracy: 7990/10000 (79.90%)\n","\n","Adjusting learning rate of group 0 to 2.2455e-02.\n","Epoch 38\n","Train: Loss=0.6411 Batch_id=390 Accuracy=76.69: 100% 391/391 [00:16<00:00, 23.30it/s]\n","Test set: Average loss: 0.0046, Accuracy: 8018/10000 (80.18%)\n","\n","Adjusting learning rate of group 0 to 1.8826e-02.\n","Epoch 39\n","Train: Loss=0.6762 Batch_id=390 Accuracy=77.22: 100% 391/391 [00:16<00:00, 23.15it/s]\n","Test set: Average loss: 0.0046, Accuracy: 7980/10000 (79.80%)\n","\n","Adjusting learning rate of group 0 to 1.5447e-02.\n","Epoch 40\n","Train: Loss=0.6427 Batch_id=390 Accuracy=77.69: 100% 391/391 [00:16<00:00, 23.81it/s]\n","Test set: Average loss: 0.0044, Accuracy: 8103/10000 (81.03%)\n","\n","Adjusting learning rate of group 0 to 1.2347e-02.\n","Epoch 41\n","Train: Loss=0.7842 Batch_id=390 Accuracy=78.07: 100% 391/391 [00:16<00:00, 23.80it/s]\n","Test set: Average loss: 0.0045, Accuracy: 8085/10000 (80.85%)\n","\n","Adjusting learning rate of group 0 to 9.5495e-03.\n","Epoch 42\n","Train: Loss=0.7769 Batch_id=390 Accuracy=78.75: 100% 391/391 [00:17<00:00, 22.42it/s]\n","Test set: Average loss: 0.0044, Accuracy: 8101/10000 (81.01%)\n","\n","Adjusting learning rate of group 0 to 7.0779e-03.\n","Epoch 43\n","Train: Loss=0.5965 Batch_id=390 Accuracy=79.01: 100% 391/391 [00:16<00:00, 23.93it/s]\n","Test set: Average loss: 0.0043, Accuracy: 8155/10000 (81.55%)\n","\n","Adjusting learning rate of group 0 to 4.9519e-03.\n","Epoch 44\n","Train: Loss=0.5270 Batch_id=390 Accuracy=79.94: 100% 391/391 [00:18<00:00, 21.35it/s]\n","Test set: Average loss: 0.0043, Accuracy: 8157/10000 (81.57%)\n","\n","Adjusting learning rate of group 0 to 3.1886e-03.\n","Epoch 45\n","Train: Loss=0.4263 Batch_id=390 Accuracy=79.95: 100% 391/391 [00:17<00:00, 22.68it/s]\n","Test set: Average loss: 0.0042, Accuracy: 8216/10000 (82.16%)\n","\n","Adjusting learning rate of group 0 to 1.8022e-03.\n","Epoch 46\n","Train: Loss=0.7096 Batch_id=390 Accuracy=80.33: 100% 391/391 [00:16<00:00, 23.50it/s]\n","Test set: Average loss: 0.0041, Accuracy: 8230/10000 (82.30%)\n","\n","Adjusting learning rate of group 0 to 8.0392e-04.\n","Epoch 47\n","Train: Loss=0.5433 Batch_id=390 Accuracy=80.78: 100% 391/391 [00:16<00:00, 23.44it/s]\n","Test set: Average loss: 0.0042, Accuracy: 8233/10000 (82.33%)\n","\n","Adjusting learning rate of group 0 to 2.0168e-04.\n","Epoch 48\n","Train: Loss=0.5548 Batch_id=390 Accuracy=80.68: 100% 391/391 [00:16<00:00, 23.46it/s]\n","Test set: Average loss: 0.0041, Accuracy: 8233/10000 (82.33%)\n","\n","Adjusting learning rate of group 0 to 4.0000e-07.\n","Epoch 49\n","Train: Loss=0.6678 Batch_id=390 Accuracy=80.82: 100% 391/391 [00:16<00:00, 23.37it/s]\n","Test set: Average loss: 0.0042, Accuracy: 8216/10000 (82.16%)\n","\n","Adjusting learning rate of group 0 to 2.0168e-04.\n","[INFO] Saving model to: models/05_going_modular_script_mode_tinyvgg_model.pth\n"]}]},{"cell_type":"code","source":["\n","!git status"],"metadata":{"id":"uKe5hKkl-Jwf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687895192122,"user_tz":-330,"elapsed":16,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}},"outputId":"dbab037d-35de-4126-90fa-e510a71c7589"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   S9.ipynb\u001b[m\n","\t\u001b[31mmodified:   src/__pycache__/data_setup.cpython-310.pyc\u001b[m\n","\t\u001b[31mmodified:   src/__pycache__/engine.cpython-310.pyc\u001b[m\n","\t\u001b[31mmodified:   src/__pycache__/model_builder.cpython-310.pyc\u001b[m\n","\t\u001b[31mmodified:   src/__pycache__/utils.cpython-310.pyc\u001b[m\n","\t\u001b[31mmodified:   src/engine.py\u001b[m\n","\t\u001b[31mmodified:   train.py\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31m../S5/__pycache__/\u001b[m\n","\t\u001b[31m../S7/__pycache__/\u001b[m\n","\t\u001b[31m../S7/data/\u001b[m\n","\t\u001b[31mmodels/\u001b[m\n","\t\u001b[31m../data/\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WRF1yTHKCSMx","executionInfo":{"status":"ok","timestamp":1687895192122,"user_tz":-330,"elapsed":9,"user":{"displayName":"Selvaraj Sembulingam","userId":"15971320449190739230"}}},"execution_count":9,"outputs":[]}]}